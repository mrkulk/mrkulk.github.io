---
title: Hierarchical value functions in deep reinforcement learning
updated: 2016-04-14 04:04
layout: post
comments: true
use_math: true
---

We just released a [paper](http://arxiv.org/abs/1604.06057) on deep reinforcement learning with hierarchical value functions. 
One of the major problems in RL is to deal with sparse reward channels. 
Without observing a non-zero reward, it is hard for any agent to learn a reasonable value function.
There is a direct relationship between the amount of exploration and observed rewards. 
Due to high branching factor in the action space, it can be difficult for the agent to efficiently explore the environment. 

This got me thinking about intrinsic notions of reward. 
My earlier [post](http://mrkulk.github.io/notes/open-questions-ai) talks about some of the prior work on intrinsic motivations. 
There has also been plenty of work on [options](https://papers.nips.cc/paper/5590-universal-option-models.pdf) in the context of reinforcement learning. 
This work is inspired by this and many other papers cited in the our arxiv paper. 

Our model called hierarchical-DQN or h-DQN integrates hierarchical value functions, operating at different temporal scales, with intrinsically motivated deep reinforcement learning. 
A top-level value function learns a policy over intrinsic goals, and a lower-level function learns a policy over atomic actions to satisfy the given goals. 
h-DQN allows for flexible goal specifications, such as functions over entities and relations. 
This provides an efficient space for exploration in complicated environments.

## Current limitations and open questions
- What is the right parametrization / representation over higher level intrinsic goals?
- How do we parse raw states into more structured representations, so that goals could be discovered in an automatic way \(especially symbolic goals\)? For pixel input, it might be a good idea to learn deep learning models that explicitly reason about objects and other structured latents in the data. There has been a push towards this direction in recent literature -- \(a\) [Ali Eslami et al.'s work](http://arkitus.com/files/arxiv-attend-infer-repeat.pdf) on disentangling objects, \(b\) [Philip Isola's work](http://arxiv.org/abs/1511.06811) on using self-supervised convnets to disentangle entities from images, \(c\) [Katerina Fragkiadaki's work](http://www.cs.berkeley.edu/~katef/papers/CVPR2015_LearnVideoSegment.pdf) on segmenting objects from videos. 
- A flexible short-term memory to handle harder semi-Markovian settings.
- Generic notions of intrinsic motivation defined over predictive models that learn to predict _what happens next_.
- Hierarchical utilities or values with growing depth
- Better exploration schemes such as the one proposed in the [bootstrapped-DQN](http://arxiv.org/pdf/1602.04621v1.pdf) paper. 
- Imagination based planning
- Using [prioritized experience replays](http://arxiv.org/abs/1511.05952) would likely enable faster learning
- An ability to handle partial-options, where the model could decide to terminate behavior towards the given goal and choose some other goal

> Tejas Kulkarni, Cambridge, MA
