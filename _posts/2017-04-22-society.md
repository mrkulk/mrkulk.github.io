---
title: Foundations of Knowledge for Achieving Cognition
updated: 2016-06-26 04:04
layout: post
comments: true
use_math: true
---

The Machine learning approach to cognition is making the most progress right now. The main proposal is to frame learning algorithms which optimize predictive questions by either reproducing or mapping information. So if you can clearly frame the questions, like winning an Atari game or playing Chess/Go, then you can apply this approach. But where do questions come from? This is the most puzzling and interesting thing and its mysteriousness alludes to the nature of abstractions. Or what are the foundations or the language to describe knowledge abstractions/representations? 

First let's define some of the words I mentioned above. We can think of abstractions as computational structures, referred to as functions or hypotheses classes depending on whether you are a Bayesian. We define a loss or likelihood function as a question dependent data quality measure -- so this is where questions come in. An inference or learning algorithm optimizes the question given various abstractions to come up with representations. Under this view, representations always emerge from a computational process. So then the main question is to come up with the "right" abstractions and questions to develop minds. 

General machine learning does not prescribe to any particular abstraction or question. Or at least not the deep reinforcement learning kind. But this is our biggest bottleneck right now to build better agents. Just as evolution has discovered these over billions of year by hacking the DNA in a multi-agent world, we will have to replicate this process efficiently by the cultural evolutionary process of science. Perhaps with a tiny bit of real evolution or whatever is computationally feasible. 

To aid in this discovery process there are various scientific works that have tried to lay down a foundation for knowledge. Foundations of human cognition were laid out by Piaget and then later refined by Spelke and other developmental psychologists. Ditributed abstractions like convolutions, recurrence, gating, attention and so on were developed in the neural networks community. The Computer Science community has discovered various abstractions like polymorphism, modularity, variables, references, search, recursion, and so on. I believe the future of AI resesarch is to integrate these abstractions together within an agent-environment framework. 


> Tejas Kulkarni, London
